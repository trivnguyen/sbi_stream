{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Training StreamSBI\n",
    "By: Tri Nguyen\n",
    "\n",
    "In this tutorial, we will train a StreamSBI model on a simple toy dataset. \n",
    "We will go over how a typical simulation-based inference (SBI) workflow looks like.\n",
    "Then, we will construct the model step-by-step, train it, and evaluate it.\n",
    "\n",
    "Before we start, make sure you have the following packages installed:\n",
    "- `numpy>=1.20`\n",
    "- `pandas`\n",
    "- `h5py`\n",
    "- `matplotlib`\n",
    "- `torch>=2.0.0`\n",
    "- `nflows`\n",
    "- `pytorch-lightning>=2.1.3`\n",
    "- `tqdm`\n",
    "- `ipython`\n",
    "- `jupyter`\n",
    "- `ml_collections`\n",
    "- `wonderwords`\n",
    "\n",
    "For this tutorial, you will also need `corner` for plotting the results. You can install these packages using\n",
    "\n",
    "```\n",
    "pip install numpy pandas h5py matplotlib torch nflows pytorch-lightning tqdm ipython jupyter ml_collections wonderwords corner\n",
    "```\n",
    "\n",
    "But I recommend installing `torch` separately using the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/), as the installation command may vary depending on your system and if you want to use GPU and CUDA. \n",
    "\n",
    "If you are on NERSC, I have prepared a Conda environment for you. You can activate it using\n",
    "\n",
    "```\n",
    "source /pscratch/sd/t/tvnguyen/envs/sbi-stream/bin/activate\n",
    "```\n",
    "\n",
    "**IMPORTANT**: After this, you might want to create your own environment. Note that PyTorch and [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) does ***NOT*** play nice with each other and can break the entire environment. If you have a simulation code using JAX, I recommend installing them in separate environments. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you will also need the data. If you are on NERSC, the tutorial data can be found at `/pscratch/sd/t/tvnguyen/stream_sbi_tutorial_data`. You should copy it to this directly or modify the path below to the correct file. The tutorial data can also be downloaded from:\n",
    "\n",
    "https://www.dropbox.com/scl/fo/qbw9282q1oxkqi6nccppf/AGZfegn9h82UlDZlgMbrl7U?rlkey=7c6u08qr7henhr7sdxbm6bpcq&dl=0\n",
    "\n",
    "If you are on NERSC and want to use the full dataset, feel free to look at our public data at `/pscratch/sd/t/tvnguyen/stream_sbi/datasets` and change the path to the dataset you want to use. At the moment, only the `2params-n1000` and `6params-n1000` are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "\n",
    "sys.path.append('..')  # add sbi_stream parent directory to path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import datasets\n",
    "from models import models, regressor, models_utils, infer_utils, utils\n",
    "from nflows import distributions, flows, transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "We will start by reading in a toy dataset and preprocessing it using a simple binning procedure.\n",
    "\n",
    "The tutorial dataset is a toy dataset with 1000 stream samples. Each stream consists of 12,000 particles, each with 6 coordinate features $(\\phi_1, \\phi_2, \\mu_1, \\mu_2, v_r, d)$. The dataset is stored in HDF5 format.\n",
    "\n",
    "The true parameters of the streams are stored in a separate CSV file. For this tutorial, we will infer 2 parameters $(M_{\\rm sat}, v_z)$, where $M_{\\rm sat}$ is the subhalo mass and $v_z$ is the impact velocity perpendicular to the stream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "data, _ = datasets.io_utils.read_dataset('/pscratch/sd/t/tvnguyen/stream_sbi_tutorial_data/data.hdf5')\n",
    "labels = pd.read_csv('/pscratch/sd/t/tvnguyen/stream_sbi_tutorial_data/labels.csv')  # also read the label\n",
    "num_streams = len(data['phi1'])\n",
    "\n",
    "print(f'Data fields: {list(data.keys())}')\n",
    "print(f'Label fields: {list(labels.columns)}')\n",
    "print(f'Number of streams: {num_streams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example raw data \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "ax.scatter(\n",
    "    data['phi1'][0], data['phi2'][0], cmap='viridis', c=data['dist'][0], s=1, alpha=0.6)\n",
    "ax.set_xlabel(r'$\\phi_1$')\n",
    "ax.set_ylabel(r'$\\phi_2$')\n",
    "cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
    "cbar.set_label('Distance [kpc]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the prior distribution\n",
    "\n",
    "In SBI, and machine learning in general, the training dataset represents the prior distribution of the parameters of interest, i.e. $(M_{\\rm sat}, v_z)$. \n",
    "\n",
    "In this tutorial, we will use a uniform prior for $v_z$ and a log-uniform prior for $M_{\\rm sat}$. In practice, the prior distribution can be more complex, e.g. a mixture of Gaussians, and can be learned from the data (in case of sequential SBI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_M_sat is not stored in the dataset, so we will calculate it\n",
    "labels['log_M_sat'] = np.log10(labels['M_sat'])\n",
    "\n",
    "# plot the distribution of log_M_sat and vz\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(labels['log_M_sat'], bins=20, histtype='step', color='k')\n",
    "axes[1].hist(labels['vz'], bins=20, histtype='step', color='k')\n",
    "\n",
    "axes[0].set_xlabel(r'$\\log_{10}(M_{\\rm sat})$')\n",
    "axes[0].set_ylabel('Number of streams')\n",
    "axes[1].set_xlabel(r'$v_z \\, [{\\rm km/s}]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tutorial, we will apply a simple data preprocessing procedure to the dataset. Of course, in practice, you would use a more sophisticated data preprocessing pipeline. The data preprocessing steps are as follows:\n",
    "\n",
    "- Bin the data into 10 bins along the $\\phi_1$ coordinate\n",
    "- Compute the mean and standard devitation of the data in each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bin the data, we can use the bin stream function in `datasets.preprocess_utils`\n",
    "# below is the copied code from the function if you want to see how it works\n",
    "def bin_stream(\n",
    "    phi1: np.ndarray, feat: np.ndarray, num_bins: int,\n",
    "    phi1_min: float = None, phi1_max: float = None\n",
    "):\n",
    "    \"\"\" Bin the stream along the phi1 coordinates and compute the mean and stdv\n",
    "    of the features in each bin. \"\"\"\n",
    "\n",
    "    phi1_min = phi1_min or phi1.min()\n",
    "    phi1_max = phi1_max or phi1.max()\n",
    "    phi1_bins = np.linspace(phi1_min, phi1_max, num_bins + 1)\n",
    "    phi1_bin_centers = 0.5 * (phi1_bins[1:] + phi1_bins[:-1])\n",
    "\n",
    "    feat_mean = np.zeros((num_bins, feat.shape[1]))\n",
    "    feat_stdv = np.zeros((num_bins, feat.shape[1]))\n",
    "    for i in range(num_bins):\n",
    "        mask = (phi1 >= phi1_bins[i]) & (phi1 <= phi1_bins[i + 1])\n",
    "        if mask.sum() <= 1:\n",
    "            continue\n",
    "        feat_mean[i] = feat[mask].mean(axis=0)\n",
    "        feat_stdv[i] = feat[mask].std(axis=0)\n",
    "\n",
    "    mask = (feat_stdv.sum(axis=1) != 0)\n",
    "    phi1_bin_centers = phi1_bin_centers[mask]\n",
    "    feat_mean = feat_mean[mask]\n",
    "    feat_stdv = feat_stdv[mask]\n",
    "\n",
    "    return phi1_bin_centers, feat_mean, feat_stdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1_min, phi1_max, num_bins = -30, 100, 10\n",
    "\n",
    "# apply binning and plot an example stream\n",
    "phi1 = data['phi1'][0]\n",
    "feat = np.stack(\n",
    "    [data[k][0] for k in ['phi2', 'pm1', 'pm2', 'vr', 'dist']], axis=1)\n",
    "\n",
    "phi1_bin_centers, feat_mean, feat_stdv = bin_stream(\n",
    "    phi1, feat, num_bins, phi1_min=phi1_min, phi1_max=phi1_max)\n",
    "\n",
    "# Plot the binned data \n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # plot the binned data with error bars\n",
    "    ax.errorbar(\n",
    "        phi1_bin_centers, feat_mean[:, i], yerr=feat_stdv[:, i], \n",
    "        fmt='o', capsize=10, zorder=10)\n",
    "    \n",
    "    # plot the scatter plot in the range\n",
    "    mask = (phi1 >= phi1_min) & (phi1 <= phi1_max)\n",
    "    ax.scatter(phi1[mask], feat[mask, i], s=1, alpha=0.05, color='gray')\n",
    "\n",
    "    ax.set_xlabel(r'$\\phi_1$', fontsize=16)\n",
    "\n",
    "axes[0].set_ylabel(r'$\\phi_2$', fontsize=16)\n",
    "axes[1].set_ylabel(r'$\\mu_1$', fontsize=16)\n",
    "axes[2].set_ylabel(r'$\\mu_2$', fontsize=16)\n",
    "axes[3].set_ylabel(r'$v_r \\, [{\\rm km/s}]$', fontsize=16)\n",
    "axes[4].set_ylabel(r'$d \\, [{\\rm kpc}]$', fontsize=16)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above example, the preprocesing procedure is very simplistic. For example, it does not take inoo account the density along the stream and sometimes the data is very sparse in some bins. In practice, you should use a more sophisticated data preprocessing pipeline, but the principle remains the same.\n",
    "\n",
    "Now, we preprocess all streams and create the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all streams and store the binned data\n",
    "x_data = []\n",
    "t_data = []   # phi1 bin centers, which we will treat differently from the other features\n",
    "y_data =  []   # labels\n",
    "for i in range(num_streams):\n",
    "    # bin data\n",
    "    phi1 = data['phi1'][i]\n",
    "    feat = np.stack(\n",
    "        [data[k][i] for k in ['phi2', 'pm1', 'pm2', 'vr', 'dist']], axis=1)    \n",
    "    phi1_bin_centers, feat_mean, feat_stdv = bin_stream(\n",
    "        phi1, feat, num_bins, phi1_min=phi1_min, phi1_max=phi1_max)\n",
    "\n",
    "    # concatenate mean and stdv into a single array\n",
    "    # this will be the final input data, with shape (num_bins, num_features *2)\n",
    "    x_data.append(np.concatenate([feat_mean, feat_stdv], axis=1))\n",
    "    t_data.append(phi1_bin_centers.reshape(-1, 1))\n",
    "    y_data.append(labels[['log_M_sat', 'vz']].values[i])\n",
    "\n",
    "# since not all the streams have the same number of bins, we need to pad the data\n",
    "# we can use the pad_and_create_mask function in `datasets.preprocess_utils`\n",
    "x_data, padding_mask = datasets.preprocess_utils.pad_and_create_mask(x_data)\n",
    "t_data, _ = datasets.preprocess_utils.pad_and_create_mask(t_data)\n",
    "y_data = np.stack(y_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out shape\n",
    "print(x_data.shape, t_data.shape, y_data.shape, padding_mask.shape)\n",
    "\n",
    "# the padding mask is a boolean array that tells us which elements are padded\n",
    "# by Pytorch conventions, it is True for padded elements\n",
    "# Note that if you are working with Jax instead, the convention is the opposite(!!)\n",
    "print(padding_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into training and validation.\n",
    "\n",
    "Additionally, in machine learning, it is common to standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "This helps the optimization algorithm to converge faster.\n",
    "Note that if you are using Neural Spline Flows (NSF, [arxiv:1906:04032](https://arxiv.org/abs/1906.04032)), you ***must*** standardize the labels because NSF requires the labels to be in a certain range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "# note that Numpy, Pytorch, and (most importantly) CUDA have their own random number generators\n",
    "# this function sets the seed for all of them\n",
    "pl.seed_everything(10)\n",
    "\n",
    "num_train = int(0.8 * num_streams)\n",
    "shuffle = np.random.permutation(num_streams)\n",
    "train_idx = shuffle[:num_train]\n",
    "val_idx = shuffle[num_train:]\n",
    "\n",
    "# calculate the normalization statistics\n",
    "# note that for the ``time'' feature, we use the min and max values instead of mean and stdv\n",
    "# first, create a mask so that we don't average over the padded elements\n",
    "mask = np.repeat(~padding_mask[train_idx, :, None], x_data.shape[-1], axis=-1)\n",
    "x_loc = x_data[train_idx].mean(axis=0, where=mask)\n",
    "x_scale = x_data[train_idx].std(axis=0, where=mask)\n",
    "y_loc = y_data[train_idx].mean(axis=0)\n",
    "y_scale = y_data[train_idx].std(axis=0)\n",
    "t_loc = t_data[train_idx].min(axis=0)\n",
    "t_scale = t_data[train_idx].max(axis=0) - t_loc\n",
    "norm_dict = {\n",
    "    'x_loc': x_loc, 'x_scale': x_scale, \n",
    "    'y_loc': y_loc, 'y_scale': y_scale,\n",
    "    't_loc': t_loc, 't_scale': t_scale\n",
    "}\n",
    "\n",
    "# normalize the data \n",
    "x_data_norm = (x_data - x_loc) / x_scale\n",
    "t_data_norm = (t_data - t_loc) / t_scale\n",
    "y_data_norm = (y_data - y_loc) / y_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytorch Dataset and DataLoader\n",
    "# first, convert the data to Pytorch tensors\n",
    "x_data_norm = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "t_data_norm = torch.tensor(t_data_norm, dtype=torch.float32)\n",
    "y_data_norm = torch.tensor(y_data_norm, dtype=torch.float32)\n",
    "padding_mask = torch.tensor(padding_mask, dtype=torch.bool)\n",
    "\n",
    "# create the dataset\n",
    "train_dset = TensorDataset(\n",
    "    x_data_norm[train_idx], y_data_norm[train_idx], t_data_norm[train_idx],\n",
    "    padding_mask[train_idx])\n",
    "val_dset = TensorDataset(\n",
    "    x_data_norm[val_idx], y_data_norm[val_idx], t_data_norm[val_idx],\n",
    "    padding_mask[val_idx])\n",
    "\n",
    "# create the dataloader\n",
    "batch_size = 64\n",
    "num_workers = 4   # number of process to load the data. Set to 0 if your data is large\n",
    "train_loader = DataLoader(\n",
    "    train_dset, batch_size=batch_size, shuffle=True,\n",
    "    pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dset, batch_size=batch_size, shuffle=False,\n",
    "    pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Constructing the model\n",
    "\n",
    "Congrats! You have successfully preprocessed the data and create a Pytorch dataset. Now, we will construct the StreamSBI model step-by-step.\n",
    "\n",
    "### Components of a typical SBI architecture\n",
    "In a typical SBI architecture, there are two main components:\n",
    "\n",
    "1. <ins>A feature extractor</ins>: a neural network that reads in the preprocessed data and outputs a fixed-size summary feature. The summary feature is a compressed representation of the data that contains all the information needed to infer the parameters of interest.\n",
    "\n",
    "2. <ins>A conditional neural posterior estimator (NPE)</ins>: a neural network that reads in the summary feature and the parameters of interest and outputs the posterior distribution of the parameters.\n",
    "\n",
    "We will train both of these components jointly. This will allow the feature extractor to learn a representation of the data that is useful for inferring the parameters of interest (and not just any representation).\n",
    "\n",
    "### The feature extractor\n",
    "The feature extractor depends on the data and the problem at hand. This is where domain knowledge comes into play (also known as the inductive bias). Because the data is a series of variable length, we will use a Transformer-based architecture, which use the self-attention mechanism to learn the relative importance between different bins in the data. For more information on Transformers, see [arxiv:1706.03762](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a copied of the TransformerFeaturizer class in `models.models.py`\n",
    "# we will use this class to featurize the input data\n",
    "class TransformerFeaturizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Featurizer based on the TransformerEncoder module from PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_feat_in,\n",
    "        d_time_in,\n",
    "        d_feat=32,\n",
    "        d_time=32,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=4,\n",
    "        dim_feedforward=128,\n",
    "        sum_features=False,\n",
    "        batch_first=True,\n",
    "        use_embedding=True,\n",
    "        activation_fn=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_feat_in : int\n",
    "            The dimension of the input features.\n",
    "        d_time_in : int\n",
    "            The dimension of the input time.\n",
    "        d_feat : int\n",
    "            The dimension of the output features.\n",
    "        d_time : int\n",
    "            The dimension of the output time.\n",
    "        nhead : int\n",
    "            The number of heads in the multiheadattention models.\n",
    "        num_encoder_layers : int\n",
    "            The number of sub-encoder-layers in the encoder.\n",
    "        sum_features : bool, optional\n",
    "            Whether to sum the features along the sequence dimension. Default: False\n",
    "        dim_feedforward : int\n",
    "            The dimension of the feedforward network model.\n",
    "        batch_first : bool, optional\n",
    "            If True, then the input and output tensors are provided as\n",
    "            (batch, seq, feature). Default: True\n",
    "        activation_fn : callable, optional\n",
    "            The activation function to use for the embedding layer. Default: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_feat = d_feat\n",
    "        self.d_time = d_time\n",
    "        self.d_model = d_feat + d_time\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.batch_first = True\n",
    "        self.use_embedding = use_embedding\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layer, num_encoder_layers)\n",
    "        self.feat_embedding_layer = nn.Linear(d_feat_in, d_feat)\n",
    "        self.time_embedding_layer = nn.Linear(d_time_in, d_time)\n",
    "\n",
    "    def forward(self, x, t, padding_mask=None):\n",
    "        x = self.feat_embedding_layer(x)\n",
    "        t = self.time_embedding_layer(t)\n",
    "        src = torch.cat((x, t), dim=-1)\n",
    "        output = self.transformer_encoder(\n",
    "            src, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # NOTE: dimension only works when batch_first=True\n",
    "        if padding_mask is None:\n",
    "            output = output.sum(dim=1)\n",
    "        else:\n",
    "            if not self.training:\n",
    "                # apply correct padding mask for evaluation\n",
    "                # this happens because with both self.eval() and torch.no_grad()\n",
    "                # the transformer encoder changes the length of the output to\n",
    "                # match the max non-padded length in the batch\n",
    "                max_seq_len = output.shape[1]\n",
    "                padding_mask = padding_mask[:, :max_seq_len]\n",
    "            output = output.masked_fill(padding_mask.unsqueeze(-1), 0)\n",
    "            output = output.sum(dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The conditional NPE\n",
    "\n",
    "For the conditional NPE, we will use a normalizing flow. Normalizing flows are a class of generative models that learn a bijective mapping between a simple distribution (e.g. a Gaussian) and the target distribution (e.g. the posterior). This allows us to efficiently sample the posterior and compute the likelihood of the data under the posterior.\n",
    "\n",
    "Mathematically, a normalizing flow is a composition of invertible transformations $f_k$:\n",
    "\n",
    "$$\n",
    "z = f_K \\circ f_{K-1} \\circ \\ldots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "where $z$ is the latent variable and $x$ is the observed data. The likelihood of the data under the posterior is given by:\n",
    "\n",
    "$$\n",
    "p(x) = p(z) \\left| \\det \\left( \\frac{\\partial f_K \\circ f_{K-1} \\circ \\ldots \\circ f_1(x)}{\\partial x} \\right) \\right| = \\prod_{k=1}^K p(z_k) \\left| \\det \\left( \\frac{\\partial f_k}{\\partial z_{k-1}} \\right) \\right|\n",
    "$$\n",
    "\n",
    "where $p(z)=p(z_1)$ is the prior distribution of the latent variable (e.g., a standard Gaussian distribution) and $\\det$ is the determinant of the Jacobian matrix of the transformation.\n",
    "The transformation $f_k$ is typically a neural network with learnable parameters.\n",
    "The normalizing flow is trained by minimizing the negative log-likelihood of the data, i.e. $\\log p(x)$, using stochastic gradient descent.\n",
    "\n",
    "In case of SBI, the flow is conditioned on the observed data (e.g. the binned data) and the parameters of interest (e.g. $(M_{\\rm sat}, v_z)$). The likelihood is given by:\n",
    "\n",
    "$$\n",
    "p(M_{\\rm sat}, v_z | x) = \\prod_{k=1}^K p(z_k | x) \\left| \\det \\left( \\frac{\\partial f_k}{\\partial z_{k-1}} \\right) \\right| \\; \\mathrm{where} \\; z_k = f_k(z_{k-1}; x)\n",
    "\n",
    "$$\n",
    "Each tranformation $f_k$ is now also conditioned on the observed data $x$.\n",
    "Including the feature extractor, the observed data $x$ is simply replaced by output of the feature extractor $s$, which depends on the data $x$.\n",
    "\n",
    "\n",
    "Here, we use Masked Autoregressive Flow (MAF) [[arxiv:1705.07057](https://arxiv.org/abs/1705.07057)] transformations implemented by the `nflow` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_maf(\n",
    "        features: int, hidden_features: int, context_features: int,\n",
    "        num_layers: int, num_blocks: int, activation_fn: callable = nn.Tanh(),\n",
    "        batch_norm: bool = True\n",
    "    ) -> flows.Flow:\n",
    "    \"\"\" Build a MAF normalizing flow\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features: int\n",
    "        Number of features\n",
    "    hidden_features: int\n",
    "        Number of hidden features\n",
    "    context_features: int\n",
    "        Number of context features\n",
    "    num_layers: int\n",
    "        Number of layers\n",
    "    num_blocks: int\n",
    "        Number of blocks\n",
    "    activation: str\n",
    "        Name of the activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    maf: flows.Flow\n",
    "        MAF normalizing flow\n",
    "    \"\"\"\n",
    "    transform = []\n",
    "    transform.append(transforms.CompositeTransform(\n",
    "        [\n",
    "            transforms.CompositeTransform(\n",
    "                [\n",
    "                    transforms.MaskedAffineAutoregressiveTransform(\n",
    "                        features=features,\n",
    "                        hidden_features=hidden_features,\n",
    "                        context_features=context_features,\n",
    "                        num_blocks=num_blocks,\n",
    "                        use_residual_blocks=False,\n",
    "                        random_mask=False,\n",
    "                        activation=activation_fn,\n",
    "                        dropout_probability=0.0,\n",
    "                        use_batch_norm=batch_norm,\n",
    "                    ),\n",
    "                    transforms.RandomPermutation(features=features),\n",
    "                ]\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    ))\n",
    "    transform = transforms.CompositeTransform(transform)\n",
    "    distribution = distributions.StandardNormal((features,))\n",
    "    maf = flows.Flow(transform, distribution)\n",
    "    return maf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, the model architecture looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the featurizer and the flows\n",
    "featurizer = TransformerFeaturizer(\n",
    "    d_feat_in=10,  # number of input features (i.e., mean and std of 5 variables)\n",
    "    d_time_in=1,  # number of input ``time'' features (i.e. phi1)\n",
    "    d_feat=16,\n",
    "    d_time=16,\n",
    "    nhead=4,  # number of heads in the multiheadattention models\n",
    "    num_encoder_layers=2,\n",
    "    dim_feedforward=32,\n",
    "    batch_first=True,  # always True\n",
    ")\n",
    "flows_model = build_maf(\n",
    "    features=2, \n",
    "    hidden_features=64,\n",
    "    context_features=32,   # dimension of the summary features, must match dim_feedforward in the Transformer\n",
    "    num_layers=4,  # number of MAF layers\n",
    "    num_blocks=2,  # number of MADE blocks in each MAF layer\n",
    "    activation_fn=nn.Tanh()\n",
    ")\n",
    "npe = nn.ModuleList([featurizer, flows_model])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model\n",
    "\n",
    "We will now train both the featurizer and the flow jointly. As mentioned, this will allow the featurizer to learn a representation of the data that is useful specifically for inferring the parameters of interest.\n",
    "\n",
    "In this tutorial, we will show a step-by-step training process using PyTorch. The `train.py` function uses PyTorch Lightning, which streamlines the training process and also provides useful training statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(npe, batch, optimizer, device='cpu'):\n",
    "    \"\"\" One simple training step \"\"\"\n",
    "    npe.train()\n",
    "    optimizer.zero_grad()\n",
    "    featurizer, flows_model = npe\n",
    "\n",
    "    # Move the batch to the device\n",
    "    x, y, t, padding_mask = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    t = t.to(device)\n",
    "    padding_mask = padding_mask.to(device)\n",
    "\n",
    "    # Compute the summary features, also the \"context\" or conditioning features of the flows\n",
    "    context = featurizer(x, t, padding_mask=padding_mask)\n",
    "\n",
    "    # log likelihood and loss\n",
    "    loglike = flows_model.log_prob(y, context=context)\n",
    "    loss = -loglike.mean()   # average over the batch\n",
    "\n",
    "    # gradient descent\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer for the gradient descent with 0.001 learning rate \n",
    "optimizer = torch.optim.Adam(npe.parameters(), lr=1e-3)\n",
    "num_epochs = 50  # number of iterations over the training data\n",
    "\n",
    "# check if GPU is available and use it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "npe = npe.to(device)\n",
    "\n",
    "# Training loop\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch\n",
    "    train_loss_ep = 0\n",
    "    npe.train()\n",
    "    for batch in train_loader:\n",
    "        loss = training_step(npe, batch, optimizer, device=device)\n",
    "        train_loss_ep += loss\n",
    "\n",
    "    # validate the model\n",
    "    val_loss_ep = 0\n",
    "    with torch.no_grad():\n",
    "        npe.eval()\n",
    "        for batch in val_loader:\n",
    "            x, y, t, padding_mask = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            t = t.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            context = featurizer(x, t, padding_mask=padding_mask)\n",
    "            loglike = flows_model.log_prob(y, context=context)\n",
    "            loss = -loglike.mean()\n",
    "            val_loss_ep += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training loss: {train_loss_ep:.4f} - Validation loss: {val_loss_ep:.4f}\")\n",
    "\n",
    "    train_loss_hist.append(train_loss_ep)\n",
    "    val_loss_hist.append(val_loss_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss_hist, label='Training loss')\n",
    "ax.plot(val_loss_hist, label='Validation loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Posterior inference\n",
    "\n",
    "After training the model, we can now perform posterior inference. We will use the trained model to infer the posterior distribution of the parameters of interest $(M_{\\rm sat}, v_z)$ given the observed data.Because the flow is a bijective transformation, we can sample from the posterior by first sampling from the prior distribution and then applying the inverse transformation of the flow. This is done using the `flow_models.sample` function.\n",
    "\n",
    "We will sample 10000 posterior samples from each stream of the validation dataset. An advantage of using the flow-based posterior estimator is that we can efficiently sample from the posterior distribution, without the need for any retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 10000\n",
    "\n",
    "with torch.no_grad():\n",
    "    npe.eval()\n",
    "    posteriors = []\n",
    "    truths = []\n",
    "    for batch in val_loader:\n",
    "        x, y, t, padding_mask = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        t = t.to(device)\n",
    "        padding_mask = padding_mask.to(device)\n",
    "        context = featurizer(x, t, padding_mask=padding_mask)\n",
    "\n",
    "        # sample and store the posterior\n",
    "        posterior = flows_model.sample(num_posteriors, context=context)\n",
    "        posterior = posterior.cpu().numpy()  # move back to CPU and convert to numpy\n",
    "        posteriors.append(posterior)\n",
    "\n",
    "        # also store the true values\n",
    "        truths.append(y.cpu().numpy())\n",
    "\n",
    "    posteriors = np.concatenate(posteriors, axis=0)\n",
    "    truths = np.concatenate(truths, axis=0)\n",
    "\n",
    "print(posteriors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we normalized the data, we need to denormalize it\n",
    "posteriors = posteriors * norm_dict['y_scale'] + norm_dict['y_loc']\n",
    "truths = truths * norm_dict['y_scale'] + norm_dict['y_loc']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `posteriors` array inlcude all the posteriors of the streams in the validation dataset. We will choose a few streams and plot the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    posteriors[0], \n",
    "    truths=truths[0],\n",
    "    truth_color='r', \n",
    "    levels=[0.68, 0.95],\n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    "    labels=[r'$\\log M_{\\rm star}$', r'$v_z$'],\n",
    "    show_titles=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    posteriors[1], \n",
    "    truths=truths[1],\n",
    "    truth_color='r', \n",
    "    levels=[0.68, 0.95],\n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    "    labels=[r'$\\log M_{\\rm star}$', r'$v_z$'],\n",
    "    show_titles=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    posteriors[2], \n",
    "    truths=truths[2],\n",
    "    truth_color='r', \n",
    "    levels=[0.68, 0.95],\n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    "    labels=[r'$\\log M_{\\rm star}$', r'$v_z$'],\n",
    "    show_titles=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posteriors are a bit noisy because we only use 800 training samples. In practice, you should use more training samples for smoother and better-calibrated posteriors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using StreamSBI \n",
    "\n",
    "StreamSBI comes with the `train.py` script that does all of the steps above. You can run the script with the following command:\n",
    "\n",
    "```bash\n",
    "python train.py --config config.py\n",
    "```\n",
    "\n",
    "We use the `ml_collections` package to manage the hyperparameters and settings for the training. The `config.py` file contains all the hyperparameters and settings for the training. You can modify the `config.py` file to change the hyperparameters and settings. An example `config.py` is provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import ConfigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    cfg = ConfigDict()\n",
    "\n",
    "    cfg.workdir = './tutorial_simple_npe'\n",
    "\n",
    "    # training loop configuration\n",
    "    cfg.num_epochs = 100\n",
    "    cfg.patience = 100\n",
    "    cfg.monitor = 'val_loss'\n",
    "    cfg.mode = 'min'\n",
    "    cfg.grad_clip = 0.5\n",
    "    cfg.save_top_k = 5\n",
    "    cfg.accelerator = 'gpu'\n",
    "\n",
    "    # model cfg\n",
    "    cfg.output_size = 2\n",
    "    cfg.featurizer = ConfigDict()\n",
    "    cfg.featurizer.name = 'transformer'\n",
    "    cfg.featurizer.d_feat_in = 10\n",
    "    cfg.featurizer.d_time_in = 1\n",
    "    cfg.featurizer.d_feat = 16\n",
    "    cfg.featurizer.d_time = 16\n",
    "    cfg.featurizer.nhead = 4\n",
    "    cfg.featurizer.num_encoder_layers = 2\n",
    "    cfg.featurizer.dim_feedforward = 32\n",
    "    cfg.featurizer.batch_first = True\n",
    "    cfg.featurizer.activation = ConfigDict()\n",
    "    cfg.featurizer.activation.name = 'Identity'\n",
    "    cfg.flows = ConfigDict()\n",
    "    cfg.flows.name = 'maf'\n",
    "    cfg.flows.hidden_size = 64\n",
    "    cfg.flows.num_blocks = 2\n",
    "    cfg.flows.num_layers = 4\n",
    "    cfg.flows.activation = ConfigDict()\n",
    "    cfg.flows.activation.name = 'tanh'\n",
    "\n",
    "    # optimizer and scheduler cfg\n",
    "    cfg.optimizer = ConfigDict()\n",
    "    cfg.optimizer.name = 'AdamW'\n",
    "    cfg.optimizer.lr = 3e-4\n",
    "    cfg.optimizer.betas = (0.9, 0.98)\n",
    "    cfg.optimizer.weight_decay = 1e-4\n",
    "    cfg.optimizer.eps = 1e-9\n",
    "    cfg.scheduler = ConfigDict()\n",
    "    cfg.scheduler.name = 'WarmUpCosineAnnealingLR'\n",
    "    cfg.scheduler.decay_steps = 100_000  # include warmup steps\n",
    "    cfg.scheduler.warmup_steps = 5000\n",
    "    cfg.scheduler.eta_min = 1e-6\n",
    "    cfg.scheduler.interval = 'step'\n",
    "\n",
    "    return cfg\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train.py` will use Pytorch Lightning to manage the training process. It will automatically save the trained model and the training statistics to the `config.workdir` specified in the `config.py` file.\n",
    "\n",
    "Below, I have copied most of the code from `train.py`, minus the data processing steps in case you want to adopt the StreamSBI framework to your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NPE regressor, which combines the featurizer and the normalizing flow\n",
    "# Note that all the training steps above, including the training loop, are now in this Regressor class\n",
    "model = regressor.Regressor(\n",
    "    output_size=config.output_size,\n",
    "    featurizer_args=config.featurizer,\n",
    "    flows_args=config.flows,\n",
    "    optimizer_args=config.optimizer,\n",
    "    scheduler_args=config.scheduler,\n",
    "    norm_dict=norm_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Lightning trainer object, which will handle the training\n",
    "callbacks = [\n",
    "    pl.callbacks.EarlyStopping(\n",
    "        monitor=config.monitor, patience=config.patience, mode=config.mode,\n",
    "        verbose=True),\n",
    "    pl.callbacks.ModelCheckpoint(\n",
    "        monitor=config.monitor, save_top_k=config.save_top_k,\n",
    "        mode=config.mode, save_weights_only=False),\n",
    "    pl.callbacks.LearningRateMonitor(\"step\"),\n",
    "]\n",
    "train_logger = pl_loggers.TensorBoardLogger(config.workdir, version='')\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=config.workdir,\n",
    "    max_epochs=config.num_epochs,\n",
    "    accelerator=config.accelerator,\n",
    "    callbacks=callbacks,\n",
    "    logger=train_logger,\n",
    "    enable_progress_bar=config.get(\"enable_progress_bar\", True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "print(\"Training model...\")\n",
    "checkpoint_path = None   # set to a path if you want to load a checkpoint, here we start from scratch\n",
    "trainer.fit(\n",
    "    model, train_loader, val_loader,\n",
    "    ckpt_path=checkpoint_path\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load in the model and posterior inference. You can load the model using `model = StreamSBI.load_from_checkpoint(checkpoint_path)`.\n",
    "The posterior inference can be done using the `models.infer_utils.sample` function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you will have to change the path to the checkpoint file, as it will be different for you\n",
    "checkpoint_path = './tutorial_simple_npe/lightning_logs/checkpoints/epoch=99-step=1300.ckpt'\n",
    "\n",
    "# Load the trained model from the checkpoint\n",
    "# map_location is used to load the model on the correct device\n",
    "# by default if the model is trained on GPU, it will be loaded on GPU if available\n",
    "model = regressor.Regressor.load_from_checkpoint(\n",
    "    checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors, truths = infer_utils.sample(\n",
    "    model, val_loader, num_samples=num_posteriors, return_labels=True,\n",
    "    norm_dict=norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an example corner plot\n",
    "fig = corner.corner(\n",
    "    posteriors[0], \n",
    "    truths=truths[0],\n",
    "    truth_color='r', \n",
    "    levels=[0.68, 0.95],\n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    "    labels=[r'$\\log M_{\\rm star}$', r'$v_z$'],\n",
    "    show_titles=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the posteriors look a bit weird here because we've overfitted the model to a small number of streams. The overfitting is a bit worse than above because we let the model run for more epochs (as well as using more advanced optimize and scheduler that will help the model converge faster)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
